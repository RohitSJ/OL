{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0JLUrDflh0o"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
        "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
        "print (nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gz_FBIIlw2-",
        "outputId": "2776b14a-6fa2-4fa3-a328-596fa7daaf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "\n",
        "line = \"The price\\t of #burgerüçî \\nin BurgerKing is Rs.36.\\n\"\n",
        "\n",
        "wh = tk.tokenize(line)\n",
        "\n",
        "print(wh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOlG1fDTmkD-",
        "outputId": "2035f9ae-c4fc-4b81-c4eb-6e476091db98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', '#burgerüçî', 'in', 'BurgerKing', 'is', 'Rs.36.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tk = WordPunctTokenizer()\n",
        "line = \"Earthüåè #is%* a \\nplanet!\"\n",
        "pun = tk.tokenize(line)\n",
        "\n",
        "print(pun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bZe6S7hobqz",
        "outputId": "add81cfb-c0cb-4e00-8352-b7bb66a5b145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Earth', 'üåè', '#', 'is', '%*', 'a', 'planet', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "line = \"Earthüåè #is% a \\nplanet!\"\n",
        "\n",
        "# Use tokenize method\n",
        "tw = tk.tokenize(line)\n",
        "\n",
        "print(tw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bENrXg9ipnUm",
        "outputId": "02e35797-ccef-416f-c19c-4b3f17fbe3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Earth', 'üåè', '#is', '%', 'a', 'planet', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
        "s = \"They'll save and investüíª more don't.\"\n",
        "#d = TreebankWordDetokenizer()\n",
        "t = TreebankWordTokenizer()\n",
        "toks = t.tokenize(s)\n",
        "#d.detokenize(toks)\n",
        "print(toks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw59tpF9ru2W",
        "outputId": "c614ad5e-d097-416a-e89f-eaf0dfcf61b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['They', \"'ll\", 'save', 'and', 'investüíª', 'more', 'do', \"n't\", '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe_list = [\"natural language processing\", \"machine learning\"]\n",
        "mwe_tokenizer = MWETokenizer(mwe_list)\n",
        "sentence = \"Natural language processing is a field of computer scienceüíª that deals with the interaction between computers and human language.\"\n",
        "tokens = mwe_tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3MQ5CuOsz3_",
        "outputId": "8c6a16b5-5663-42fb-f8b4-1591df420479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'e', 'l', 'd', ' ', 'o', 'f', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'üíª', ' ', 't', 'h', 'a', 't', ' ', 'd', 'e', 'a', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', 'i', 'o', 'n', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '.']\n",
            "129\n"
          ]
        }
      ]
    }
  ]
}